{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashleybrea/06-AIT-HW/blob/main/Copy_of_09_Assigment_6_text_generation_AshleyBrea.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copyright\n",
        "\n",
        "<PRE>\n",
        "Copyright (c) Bálint Gyires-Tóth - All Rights Reserved\n",
        "You may use and modify this code for research and development purpuses.\n",
        "Using this code for educational purposes (self-paced or instructor led) without the permission of the author is prohibited.\n",
        "</PRE>"
      ],
      "metadata": {
        "id": "CtuSrazlNYEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment: RNN text generation with your favorite book\n"
      ],
      "metadata": {
        "id": "vriXNd_nL2q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Dataset\n",
        "- Download your favorite book from https://www.gutenberg.org/\n",
        "- Combine all sonnets into a single text source.  \n",
        "- Split into training (80%) and validation (20%).  "
      ],
      "metadata": {
        "id": "Q5atve1sMH9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://www.gutenberg.org/cache/epub/514/pg514.txt\"\n",
        "response = requests.get(url)\n",
        "text = response.text\n",
        "len(text)\n"
      ],
      "metadata": {
        "id": "QvKdt5EyMDug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b843f491-8eb8-4619-cdf7-71ed03f3c597"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1047545"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "split_index = int(0.8 * len(text))\n",
        "train_text = text[:split_index]\n",
        "val_text = text[split_index:]\n",
        "\n",
        "print(f\"Training text length: {len(train_text)}\")\n",
        "print(f\"Validation text length: {len(val_text)}\")"
      ],
      "metadata": {
        "id": "W8lKEv8VEM0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bf77372-80b0-4078-d711-0b72cedefdd5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training text length: 838036\n",
            "Validation text length: 209509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Preprocessing\n",
        "- Convert text to lowercase.  \n",
        "- Remove punctuation (except basic sentence delimiters).  \n",
        "- Tokenize by words or characters (your choice).  \n",
        "- Build a vocabulary (map each unique word to an integer ID)."
      ],
      "metadata": {
        "id": "4eQMcyPgMLJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def data_preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\wls.!?]', ' ', text)\n",
        "    tokens = re.findall(r'\\b\\w+\\b|[.!?]', text)\n",
        "    return tokens\n",
        "\n",
        "train_tokens = data_preprocess(train_text)\n",
        "val_tokens = data_preprocess(val_text)\n",
        "\n",
        "vocab = {word: idx for idx, word in enumerate(sorted(set(train_tokens)))}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "def making_X_and_Y(token_list, vocab, window_size) :\n",
        "    token_ids = [vocab[token] for token in token_list if token in vocab]\n",
        "    X = []\n",
        "    Y = []\n",
        "    for i in range(len(token_ids) - window_size):\n",
        "        X.append(token_ids[i:i + window_size])\n",
        "        Y.append(token_ids[i + window_size])\n",
        "\n",
        "    return np.array(X), np.array(Y)\n",
        "\n",
        "sequence_length = 6\n",
        "X_train, Y_train = making_X_and_Y(train_tokens, vocab, sequence_length)\n",
        "X_val, Y_val = making_X_and_Y(val_tokens, vocab, sequence_length)\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, Y_train shape: {Y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, Y_val shape: {Y_val.shape}\")\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(\"Example X:\", X_train[0])\n",
        "print(\"Example Y:\", Y_train[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08SIbJetUZhy",
        "outputId": "fd796773-8478-4ed7-efbc-5523d1182e80"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (164886, 6), Y_train shape: (164886,)\n",
            "X_val shape: (38484, 6), Y_val shape: (38484,)\n",
            "(164886, 6) (164886,)\n",
            "Example X: [8705 6673 3884 2735 5857 5021]\n",
            "Example Y: 9656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Embedding Layer in Keras\n",
        "Below is a minimal example of defining an `Embedding` layer:\n",
        "```python\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer = Embedding(\n",
        "    input_dim=vocab_size,     # size of the vocabulary\n",
        "    output_dim=128,           # embedding vector dimension\n",
        "    input_length=sequence_length\n",
        ")\n",
        "```\n",
        "- This layer transforms integer-encoded sequences (word IDs) into dense vector embeddings.\n",
        "\n",
        "- Feed these embeddings into your LSTM or GRU OR 1D CNN layer."
      ],
      "metadata": {
        "id": "jbTZs3OiMMNy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model\n",
        "- Implement an LSTM or GRU or 1D CNN-based language model with:\n",
        "  - **The Embedding layer** as input.\n",
        "  - At least **one recurrent layer** (e.g., `LSTM(256)` or `GRU(256)` or your custom 1D CNN).\n",
        "  - A **Dense** output layer with **softmax** activation for word prediction.\n",
        "- Train for about **5–10 epochs** so it can finish in approximately **2 hours** on a standard machine.\n"
      ],
      "metadata": {
        "id": "qsXR4RZpMXMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# my model seems to be overfitting so implementing early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=128, input_length=sequence_length))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "Q_OBDDt4tsQu",
        "outputId": "32391a7e-f6f3-43d9-87e0-4ed453e8c03c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training & Evaluation\n",
        "- **Monitor** the loss on both training and validation sets.\n",
        "- **Perplexity**: a common metric for language models.\n",
        "  - It is the exponent of the average negative log-likelihood.\n",
        "  - If your model outputs cross-entropy loss `H`, then `perplexity = e^H`.\n",
        "  - Try to keep the validation perplexity **under 50** if possible."
      ],
      "metadata": {
        "id": "Ggop4h4IMhMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, batch_size=64, epochs=10, validation_data=(X_val, Y_val), callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2iVuvroyoz1",
        "outputId": "a0dee9ba-d6e5-4d5f-b69c-c1c60a575fc3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m314s\u001b[0m 120ms/step - accuracy: 0.0564 - loss: 6.5421 - val_accuracy: 0.1071 - val_loss: 5.7509\n",
            "Epoch 2/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m319s\u001b[0m 119ms/step - accuracy: 0.1192 - loss: 5.5853 - val_accuracy: 0.1270 - val_loss: 5.5200\n",
            "Epoch 3/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 118ms/step - accuracy: 0.1453 - loss: 5.2243 - val_accuracy: 0.1368 - val_loss: 5.4321\n",
            "Epoch 4/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m303s\u001b[0m 117ms/step - accuracy: 0.1631 - loss: 4.9573 - val_accuracy: 0.1404 - val_loss: 5.4121\n",
            "Epoch 5/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m317s\u001b[0m 115ms/step - accuracy: 0.1786 - loss: 4.7125 - val_accuracy: 0.1441 - val_loss: 5.4288\n",
            "Epoch 6/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m325s\u001b[0m 117ms/step - accuracy: 0.1916 - loss: 4.4833 - val_accuracy: 0.1451 - val_loss: 5.4793\n",
            "Epoch 7/10\n",
            "\u001b[1m2577/2577\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m321s\u001b[0m 116ms/step - accuracy: 0.2071 - loss: 4.2515 - val_accuracy: 0.1454 - val_loss: 5.5429\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7e917f1daa90>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loss, val_acc = model.evaluate(X_val, Y_val)\n",
        "\n",
        "print(\"Val Perplexity: \", np.exp(val_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgZVYddqtjmT",
        "outputId": "82f6ae4c-69ee-42b4-ffdc-bb7d70f13099"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1203/1203\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 22ms/step - accuracy: 0.1468 - loss: 5.2749\n",
            "Val Perplexity:  224.09803801008024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Generation Criteria\n",
        "- After training, generate **two distinct text samples**, each at least **50 tokens**.\n",
        "- Use **different seed phrases** (e.g., “love is” vs. “time will”)."
      ],
      "metadata": {
        "id": "cbvbBOp3MfTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: make a function to generate text for a seed phrase, with at least 50 tokes, and that sample from the predicted distribution instead of always picking the highest-probability token using argmax to avoid repitition\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def generate_text(model, seed_text, vocab, vocab_inv, num_tokens_to_generate=50):\n",
        "    generated_text = seed_text\n",
        "    seed_tokens = preprocess(seed_text)\n",
        "    token_ids = [vocab[token] for token in seed_tokens if token in vocab]\n",
        "\n",
        "    for _ in range(num_tokens_to_generate):\n",
        "        padded_input = token_ids[-sequence_length:]\n",
        "        if len(padded_input) < sequence_length:\n",
        "            padded_input = [0]*(sequence_length - len(padded_input)) + padded_input\n",
        "\n",
        "        input_sequence = np.array([padded_input])\n",
        "        predicted_probs = model.predict(input_sequence, verbose=0)[0]\n",
        "\n",
        "        # Sample from the predicted distribution\n",
        "        predicted_id = np.random.choice(vocab_size, p=predicted_probs)\n",
        "\n",
        "        # Avoid repeatedly predicting the same token\n",
        "        # This is not perfect but helps with repetition reduction\n",
        "        if predicted_id == token_ids[-1]:\n",
        "          #Try to choose the next highest probability\n",
        "          second_highest = np.argsort(predicted_probs)[-2]\n",
        "          predicted_id = second_highest\n",
        "\n",
        "        token_ids.append(predicted_id)\n",
        "        generated_text += \" \" + vocab_inv[predicted_id]\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Create reverse vocabulary for lookup\n",
        "vocab_inv = {idx: word for word, idx in vocab.items()}\n"
      ],
      "metadata": {
        "id": "LHqA5SEJw_mn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, \"time will\", vocab, vocab_inv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Z2ADN_zMvCq2",
        "outputId": "226c3b9d-78e0-4d48-b194-bcb43f860ae6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time will us your coffee ? asked her mother somewhat saturday under the window poor mind that s sake this idea softly so well i got your soft in this tossed and threateningly delighted and too for a king s glove and seized the good ambition of mine . i get very'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_text(model, \"love is\", vocab, vocab_inv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "niNtqj9SvNUU",
        "outputId": "06731bc0-bdaf-4cd6-d508-b96dc5f52115"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'love is snodgrass a row down and tomorrow here when they had all toward them then at white tumbling saying pride to bed the old lady made the shining four loving neglecting which experiences flew beautifully therefore she danced years of cherished the german bill rocking fastening for a mother with all'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}